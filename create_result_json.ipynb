{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!git clone https://github.com/xinyu1205/recognize-anything.git\n","%cd recognize-anything\n","%pip install -e ."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-10T04:10:44.790955Z","iopub.status.busy":"2024-08-10T04:10:44.790195Z","iopub.status.idle":"2024-08-10T04:10:44.796387Z","shell.execute_reply":"2024-08-10T04:10:44.795386Z","shell.execute_reply.started":"2024-08-10T04:10:44.790923Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/quan238/personal/Build/competitions/AIC_2024/AIC/RAM-migration/.conda/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n","  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n","/Users/quan238/personal/Build/competitions/AIC_2024/AIC/RAM-migration/.conda/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n","  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import cv2\n","import torch\n","import os\n","from PIL import Image\n","from ram.models import ram_plus\n","from ram import inference_ram as inference\n","from ram import get_transform\n","from tqdm.notebook import tqdm\n","from typing import List, Tuple\n","from torchvision.transforms import Compose"]},{"cell_type":"markdown","metadata":{},"source":["### Hàm khởi tạo mô hình RAM++ (Recognize Anything Model)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def initialize_model(model_weight: str, image_size: int) -> Tuple[torch.nn.Module, Compose]:\n","    \"\"\"\n","    Khởi tạo mô hình RAM và bộ chuyển đổi hình ảnh.\n","\n","    Args:\n","        model_weight (str): Đường dẫn đến file trọng số của mô hình.\n","        image_size (int): Kích thước ảnh đầu vào cho mô hình.\n","\n","    Returns:\n","        Tuple[torch.nn.Module, Compose]: Mô hình đã được khởi tạo và bộ chuyển đổi hình ảnh.\n","    \"\"\"\n","    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    \n","    # Khởi tạo bộ chuyển đổi hình ảnh\n","    transform: Compose = get_transform(image_size=image_size)\n","    \n","    # Khởi tạo mô hình RAM\n","    model: torch.nn.Module = ram_plus(\n","        pretrained=model_weight,\n","        image_size=image_size,\n","        vit='swin_l'\n","    )\n","    model.eval() # chuyển qua chế dộ đánh giá (evaluation)\n","    model = model.to(device) # chuyển qua device \n","    \n","    return model, transform"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["python(30639) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"]},{"name":"stdout","output_type":"stream","text":["--2024-08-11 22:32:55--  https://huggingface.co/xinyu1205/recognize-anything-plus-model/resolve/main/ram_plus_swin_large_14m.pth\n","Resolving huggingface.co (huggingface.co)... 18.165.122.120, 18.165.122.101, 18.165.122.11, ...\n","Connecting to huggingface.co (huggingface.co)|18.165.122.120|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs-us-1.huggingface.co/repos/1f/c0/1fc0c455d992a58616eaae5f3ce9e34b1d2c49026fede34de9a7a5f3d06373dd/497c178836ba66698ca226c7895317e6e800034be986452dbd2593298d50e87d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ram_plus_swin_large_14m.pth%3B+filename%3D%22ram_plus_swin_large_14m.pth%22%3B&Expires=1723663976&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzY2Mzk3Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzFmL2MwLzFmYzBjNDU1ZDk5MmE1ODYxNmVhYWU1ZjNjZTllMzRiMWQyYzQ5MDI2ZmVkZTM0ZGU5YTdhNWYzZDA2MzczZGQvNDk3YzE3ODgzNmJhNjY2OThjYTIyNmM3ODk1MzE3ZTZlODAwMDM0YmU5ODY0NTJkYmQyNTkzMjk4ZDUwZTg3ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=mAcKihtsBhA6AyQoCR-5YUf2jzriJbJ6t%7E0VZGBdsGUeNz0blBUbee60E7GliSrex1YjZ7OKrQb2kWI7BY5XyOXHqkkKytZO5-F78z5dzWplyMSlJJNCOleMZXstdcVrWYpT8cd8rjfCxaWNGBy7BJ%7Ehras-g4ZfvZwfOMYiJbMfwTEU933s4kTnNsHRUfauVMbWbg2dkD0KFb5IVN%7E%7EFYwdnWne1atfb1CCiDVzgcjbije0lNHJBQNmOjDDc2eksACidEgdPwwhBexjQz%7ExUZY-7%7EjmKLzh7Uf9MrgbnN0f9nW1yVVzkmqB1L22X-ZwPxIP0PmGrCIz5JuYZghNkQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n","--2024-08-11 22:32:56--  https://cdn-lfs-us-1.huggingface.co/repos/1f/c0/1fc0c455d992a58616eaae5f3ce9e34b1d2c49026fede34de9a7a5f3d06373dd/497c178836ba66698ca226c7895317e6e800034be986452dbd2593298d50e87d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ram_plus_swin_large_14m.pth%3B+filename%3D%22ram_plus_swin_large_14m.pth%22%3B&Expires=1723663976&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzY2Mzk3Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzFmL2MwLzFmYzBjNDU1ZDk5MmE1ODYxNmVhYWU1ZjNjZTllMzRiMWQyYzQ5MDI2ZmVkZTM0ZGU5YTdhNWYzZDA2MzczZGQvNDk3YzE3ODgzNmJhNjY2OThjYTIyNmM3ODk1MzE3ZTZlODAwMDM0YmU5ODY0NTJkYmQyNTkzMjk4ZDUwZTg3ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=mAcKihtsBhA6AyQoCR-5YUf2jzriJbJ6t%7E0VZGBdsGUeNz0blBUbee60E7GliSrex1YjZ7OKrQb2kWI7BY5XyOXHqkkKytZO5-F78z5dzWplyMSlJJNCOleMZXstdcVrWYpT8cd8rjfCxaWNGBy7BJ%7Ehras-g4ZfvZwfOMYiJbMfwTEU933s4kTnNsHRUfauVMbWbg2dkD0KFb5IVN%7E%7EFYwdnWne1atfb1CCiDVzgcjbije0lNHJBQNmOjDDc2eksACidEgdPwwhBexjQz%7ExUZY-7%7EjmKLzh7Uf9MrgbnN0f9nW1yVVzkmqB1L22X-ZwPxIP0PmGrCIz5JuYZghNkQ__&Key-Pair-Id=K24J24Z295AEI9\n","Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 18.165.140.30, 18.165.140.18, 18.165.140.83, ...\n","Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|18.165.140.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3010210801 (2.8G) [binary/octet-stream]\n","Saving to: ‘pretrained/ram_swin_large_14m.pth’\n","\n","pretrained/ram_swin 100%[===================>]   2.80G  14.3MB/s    in 3m 31s  \n","\n","2024-08-11 22:36:27 (13.6 MB/s) - ‘pretrained/ram_swin_large_14m.pth’ saved [3010210801/3010210801]\n","\n","RAM weights are downloaded!\n"]}],"source":["def download_checkpoints():\n","    if not os.path.exists('pretrained'):\n","        os.makedirs('pretrained')\n","\n","    ram_weights_path = 'pretrained/ram_swin_large_14m.pth'\n","    if not os.path.exists(ram_weights_path):\n","        !wget https://huggingface.co/xinyu1205/recognize-anything-plus-model/resolve/main/ram_plus_swin_large_14m.pth -O pretrained/ram_swin_large_14m.pth\n","    else:\n","        print(\"RAM weights already downloaded!\")\n","\n","download_checkpoints()\n","print(model, 'weights are downloaded!')\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import os\n","import json\n","from typing import Dict, List\n","import torch\n","from tqdm import tqdm\n","from PIL import Image\n","\n","\n","def process_image(image_path: str, transform, model, device: torch.device) -> List[str]:\n","    \"\"\"Process a single image and return the list of detected objects.\"\"\"\n","    img = Image.open(image_path)\n","    processed_img = transform(img).unsqueeze(0).to(device)\n","    res = inference(processed_img, model)\n","    objects = [obj.strip() for obj in res[0].strip().split(\"|\")]\n","    return objects\n","\n","\n","def process_video(\n","    video_path: str, transform, model, device: torch.device, base_dir: str\n",") -> Dict[str, List[str]]:\n","    \"\"\"Process all images in a video folder and return a dictionary of detected objects and their image paths.\"\"\"\n","    results = {}\n","    image_paths = [\n","        os.path.join(video_path, f)\n","        for f in os.listdir(video_path)\n","        if f.lower().endswith((\"jpg\", \"jpeg\", \"png\", \"webp\"))\n","    ]\n","\n","    for image_path in tqdm(\n","        image_paths, desc=f\"Processing {os.path.relpath(video_path, base_dir)}\"\n","    ):\n","        objects = process_image(image_path, transform, model, device)\n","        relative_path = os.path.relpath(image_path, base_dir)\n","\n","        for obj in objects:\n","            if obj not in results:\n","                results[obj] = []\n","            results[obj].append(relative_path)\n","\n","    return results\n","\n","\n","def process_group(\n","    group_path: str, transform, model, device: torch.device, base_dir: str\n",") -> Dict[str, List[str]]:\n","    \"\"\"Process all videos in a group folder and return a dictionary of detected objects and their image paths.\"\"\"\n","    group_results = {}\n","\n","    for video in os.listdir(group_path):\n","        video_path = os.path.join(group_path, video)\n","        if os.path.isdir(video_path):\n","            video_results = process_video(\n","                video_path, transform, model, device, base_dir\n","            )\n","            for obj, paths in video_results.items():\n","                if obj not in group_results:\n","                    group_results[obj] = []\n","                group_results[obj].extend(paths)\n","\n","    return group_results\n","\n","\n","def create_json(\n","    model: torch.nn.Module,\n","    transform: Compose,\n","    base_dir: str = \"data/images\",\n","    output_file: str = \"results.json\",\n",") -> None:\n","    \"\"\"Traverse all video directories, run inference, and create a JSON results file.\"\"\"\n","    device: torch.device = next(model.parameters()).device\n","    results: Dict[str, List[str]] = {}\n","\n","    for group in os.listdir(base_dir):\n","        group_path = os.path.join(base_dir, group)\n","        if os.path.isdir(group_path):\n","            group_results = process_group(\n","                group_path, transform, model, device, base_dir\n","            )\n","            for obj, paths in group_results.items():\n","                if obj not in results:\n","                    results[obj] = []\n","                results[obj].extend(paths)\n","\n","    # Write results to JSON file\n","    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(results, f, ensure_ascii=False, indent=2)\n","\n","    print(f\"Results saved to {output_file}\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------\n","pretrained/ram_swin_large_14m.pth\n","--------------\n","load checkpoint from pretrained/ram_swin_large_14m.pth\n","vit: swin_l\n"]},{"name":"stderr","output_type":"stream","text":["Processing 1/6: 100%|██████████| 149/149 [02:23<00:00,  1.04it/s]\n","Processing 1/19: 100%|██████████| 44/44 [00:41<00:00,  1.05it/s]\n","Processing 2/4: 100%|██████████| 110/110 [01:47<00:00,  1.02it/s]\n","Processing 2/3: 100%|██████████| 58/58 [00:53<00:00,  1.09it/s]"]},{"name":"stdout","output_type":"stream","text":["Results saved to results.json\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["IMAGE_SIZE: int = 384\n","MODEL_WEIGHT: str = os.path.join('pretrained', 'ram_swin_large_14m.pth')\n","\n","# Khởi tạo mô hình và bộ chuyển đổi\n","model, transform = initialize_model(MODEL_WEIGHT, IMAGE_SIZE)\n","\n","# Chạy inference và tạo file JSON\n","create_json(model, transform)"]},{"cell_type":"markdown","metadata":{},"source":["# Tag list \n","\n","Tag list này để cho người dùng chọn và filter\n","\n","https://github.com/xinyu1205/recognize-anything/blob/main/ram/data/ram_tag_list.txt\n","\n","https://github.com/xinyu1205/recognize-anything/blob/main/datasets/openimages_rare_200/openimages_rare_200_ram_taglist.txt"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5488188,"sourceId":9094383,"sourceType":"datasetVersion"},{"datasetId":5514625,"sourceId":9133171,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
